name: CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}

jobs:
  test:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Go
      uses: actions/setup-go@v5
      with:
        go-version: '1.24'
        cache: true
    
    - name: Run unit tests
      run: |
        go test -v -race -coverprofile=coverage.out ./...
        go tool cover -html=coverage.out -o coverage.html
    
    - name: Upload coverage reports
      uses: codecov/codecov-action@v4
      with:
        file: ./coverage.out
        fail_ci_if_error: true
    
    - name: Run integration tests
      run: |
        go test -v -tags=integration ./...

  security-scan:
    runs-on: ubuntu-latest
    needs: test
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Run Trivy vulnerability scanner
      uses: aquasecurity/trivy-action@master
      with:
        scan-type: 'fs'
        scan-ref: '.'
        format: 'sarif'
        output: 'trivy-results.sarif'
    
    - name: Upload Trivy scan results to GitHub Security tab
      uses: github/codeql-action/upload-sarif@v3
      if: always()
      with:
        sarif_file: 'trivy-results.sarif'
    
    - name: Run gosec security scanner
      uses: securecodewarrior/github-action-gosec@master
      with:
        args: '-fmt sarif -out gosec-results.sarif ./...'
    
    - name: Upload gosec scan results
      uses: github/codeql-action/upload-sarif@v3
      if: always()
      with:
        sarif_file: 'gosec-results.sarif'

  build-and-scan:
    runs-on: ubuntu-latest
    needs: [test, security-scan]
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
    
    - name: Build Docker image
      uses: docker/build-push-action@v5
      with:
        context: .
        file: ./deploy/Dockerfile
        push: false
        tags: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ github.sha }}
        cache-from: type=gha
        cache-to: type=gha,mode=max
    
    - name: Run Trivy container scan
      uses: aquasecurity/trivy-action@master
      with:
        image-ref: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ github.sha }}
        format: 'sarif'
        output: 'trivy-container-results.sarif'
    
    - name: Upload container scan results
      uses: github/codeql-action/upload-sarif@v3
      if: always()
      with:
        sarif_file: 'trivy-container-results.sarif'

  deploy:
    runs-on: ubuntu-latest
    needs: [test, security-scan, build-and-scan]
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Log in to Container Registry
      uses: docker/login-action@v3
      with:
        registry: ${{ env.REGISTRY }}
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
    
    - name: Build and push Docker image
      uses: docker/build-push-action@v5
      with:
        context: .
        file: ./deploy/Dockerfile
        push: true
        tags: |
          ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:latest
          ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ github.sha }}
        cache-from: type=gha
        cache-to: type=gha,mode=max
    
    - name: Sign container image
      uses: sigstore/cosign-installer@v3
    
    - name: Sign the published Docker image
      env:
        COSIGN_EXPERIMENTAL: 1
      run: |
        echo "${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ github.sha }}" | xargs -I {} cosign sign --yes {}@$(docker inspect --format='{{index .RepoDigests 0}}' {})

  ml-pipeline:
    runs-on: ubuntu-latest
    needs: test
    if: github.ref == 'refs/heads/main'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.12'
        cache: 'pip'
    
    - name: Install ML dependencies
      run: |
        pip install -r scripts/requirements.txt
    
    - name: Validate ML pipeline
      run: |
        python scripts/label_and_train.py --validate-only --data-file scripts/training_data.json
    
    - name: Create test dataset for validation
      run: |
        python -c "
        import json
        import numpy as np
        
        # Generate synthetic test data
        np.random.seed(42)
        test_data = []
        for i in range(1000):
            features = np.random.randn(10).tolist()
            label = int(np.random.choice([0, 1]))
            test_data.append({'features': features, 'label': label})
        
        with open('test_data.json', 'w') as f:
            json.dump(test_data, f)
        "
    
    - name: Run model validation (if model exists)
      run: |
        if [ -f "model.onnx" ]; then
          python scripts/model_validation.py \
            --model model.onnx \
            --test-data test_data.json \
            --output validation_results.json \
            --min-accuracy 0.50 \
            --min-precision 0.45 \
            --min-recall 0.45 \
            --min-f1 0.45 \
            --min-auc 0.50
        else
          echo "No model found, skipping validation"
        fi
    
    - name: Upload ML artifacts
      if: success()
      uses: actions/upload-artifact@v4
      with:
        name: ml-models
        path: |
          *.onnx
          *_metrics.json
          validation_results.json
          model_validation.log
        retention-days: 30

  monitoring-validation:
    runs-on: ubuntu-latest
    needs: test
    if: github.ref == 'refs/heads/main'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.12'
        cache: 'pip'
    
    - name: Install monitoring dependencies
      run: |
        pip install requests prometheus-client pandas numpy
    
    - name: Validate monitoring scripts
      run: |
        python scripts/enhanced_monitoring.py --help
        python scripts/model_validation.py --help
        python -c "import scripts.enhanced_monitoring; import scripts.model_validation; print('Monitoring scripts validated successfully')"
    
    - name: Run monitoring dry-run
      run: |
        # Create mock metrics endpoint response
        mkdir -p mock_data
        echo "# HELP test_metric Test metric
        # TYPE test_metric counter
        test_metric 42" > mock_data/metrics.txt
        
        # Test monitoring script with mock data
        python -c "
        from scripts.enhanced_monitoring import MetricsCollector, AlertManager, MonitoringDashboard
        print('Monitoring components imported successfully')
        print('Monitoring validation completed')
        "
